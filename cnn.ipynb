{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "GPU used: /device:GPU:0\n",
      "-----------------------------------------\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 11180634668609264258\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3455778816\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14828398369427270334\n",
      "physical_device_desc: \"device: 0, name: Quadro P2000, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-29 13:47:54.472986: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-01-29 13:47:54.486866: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:54.513835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:54.514042: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:54.896599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:54.896776: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:54.896923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:54.897097: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 3295 MB memory:  -> device: 0, name: Quadro P2000, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2022-01-29 13:47:54.898170: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:54.898346: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:54.898476: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:54.898634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:54.898765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:54.898872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 3295 MB memory:  -> device: 0, name: Quadro P2000, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2022-01-29 13:47:54.899277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:54.899490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:54.899685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:54.899952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:54.900149: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:54.900301: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /device:GPU:0 with 3295 MB memory:  -> device: 0, name: Quadro P2000, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "import os, warnings\n",
    "import shutil\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import math\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "\n",
    "from scripts import read_saves, write_saves, record_saves\n",
    "\n",
    "def set_seed(seed=31415):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "set_seed()\n",
    "\n",
    "plt.rc('figure', autolayout=True)\n",
    "plt.rc('axes', labelweight='bold', labelsize='large',\n",
    "       titleweight='bold', titlesize=18, titlepad=10)\n",
    "plt.rc('image', cmap='magma')\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"-----------------------------------------\")\n",
    "if tf.test.gpu_device_name():\n",
    "    print(f\"GPU used: {tf.test.gpu_device_name()}\")\n",
    "else:\n",
    "    print(f\"GPU not used\")\n",
    "print(\"-----------------------------------------\")\n",
    "\n",
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 1 Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"exploration\": False,\n",
    "    \"custom\": False,\n",
    "    \"transfer\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_dir = \"data/Annotations\"\n",
    "images_dir = \"data/Images\"\n",
    "target_dir = \"data/Targets\"\n",
    "batch_size = 16\n",
    "\n",
    "SAVE_PATH = \"data/saves.pkl\"\n",
    "SAVE_NB = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 2 Dataset exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_float(image, label):\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def breeds_distribution(dataset, figsize=(25, 5), display_mean=True):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    sns.barplot(x=fre_df[\"Breed\"], y=fre_df[\"Population\"])\n",
    "\n",
    "    if display_mean:\n",
    "        mean = fre_df.mean(axis=0)[0]\n",
    "        plt.axhline(mean, linestyle=\"--\", linewidth=1, color=\"r\")\n",
    "\n",
    "    plt.title(\"Breeds distribution\", size=20)\n",
    "    plt.xticks(size=10, rotation=45, ha=\"right\")\n",
    "    plt.yticks(size=10)\n",
    "    plt.ylabel(\"Count\", size=16)\n",
    "\n",
    "    if display_mean:\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 2.1 Breeds distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many breeds are present in the dataset ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 120 different dog breeds\n"
     ]
    }
   ],
   "source": [
    "breed_subdir = os.listdir(images_dir)\n",
    "print(f\"There is {len(breed_subdir)} different dog breeds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a pandas dataset with all breeds and each of their respective population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Breed_full</th>\n",
       "      <th>Breed</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n02111889-Samoyed</td>\n",
       "      <td>Samoyed</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n02105641-Old_English_sheepdog</td>\n",
       "      <td>Old_English_sheepdog</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n02107908-Appenzeller</td>\n",
       "      <td>Appenzeller</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n02100583-vizsla</td>\n",
       "      <td>vizsla</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n02097298-Scotch_terrier</td>\n",
       "      <td>Scotch_terrier</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Breed_full                 Breed  Population\n",
       "0               n02111889-Samoyed               Samoyed         218\n",
       "1  n02105641-Old_English_sheepdog  Old_English_sheepdog         169\n",
       "2           n02107908-Appenzeller           Appenzeller         151\n",
       "3                n02100583-vizsla                vizsla         154\n",
       "4        n02097298-Scotch_terrier        Scotch_terrier         158"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breed_full: list = []\n",
    "breed: list = []\n",
    "population: list = []\n",
    "\n",
    "for sub_d in breed_subdir:\n",
    "    breed_full.append(sub_d)\n",
    "    breed.append(sub_d.split(\"-\")[1])\n",
    "    population.append(len([obs for obs in os.listdir(images_dir + \"/\" + sub_d)]))\n",
    "\n",
    "fre_df = pd.DataFrame(data={\"Breed_full\": breed_full, \"Breed\": breed, \"Population\": population})\n",
    "fre_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the breeds populations and the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"exploration\"]:\n",
    "    mean = breeds_distribution(fre_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot again bu after a sort on Population field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"exploration\"]:\n",
    "    fre_df.sort_values(by=\"Population\", inplace=True, ascending=False, axis=0)\n",
    "    mean = breeds_distribution(fre_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the top three breeds, see if they are sufficiently differents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"exploration\"]:\n",
    "\n",
    "    fix, axs = plt.subplots(3, 3, figsize=(20, 10))\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "\n",
    "    # Malteses\n",
    "    plt.subplot(3, 3, 1)\n",
    "    image = tf.io.read_file(\"data/Images/n02085936-Maltese_dog/n02085936_37.jpg\")\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "    img = tf.squeeze(image).numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(3, 3, 2)\n",
    "    image = tf.io.read_file(\"data/Images/n02085936-Maltese_dog/n02085936_66.jpg\")\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "    img = tf.squeeze(image).numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(3, 3, 3)\n",
    "    image = tf.io.read_file(\"data/Images/n02085936-Maltese_dog/n02085936_233.jpg\")\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "    img = tf.squeeze(image).numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Afghan\n",
    "    plt.subplot(3, 3, 4)\n",
    "    image = tf.io.read_file(\"data/Images/n02088094-Afghan_hound/n02088094_231.jpg\")\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "    img = tf.squeeze(image).numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(3, 3, 5)\n",
    "    image = tf.io.read_file(\"data/Images/n02088094-Afghan_hound/n02088094_251.jpg\")\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "    img = tf.squeeze(image).numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(3, 3, 6)\n",
    "    image = tf.io.read_file(\"data/Images/n02088094-Afghan_hound/n02088094_272.jpg\")\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "    img = tf.squeeze(image).numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Scottish deerhound\n",
    "    plt.subplot(3, 3, 7)\n",
    "    image = tf.io.read_file(\"data/Images/n02092002-Scottish_deerhound/n02092002_3.jpg\")\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "    img = tf.squeeze(image).numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(3, 3, 8)\n",
    "    image = tf.io.read_file(\"data/Images/n02092002-Scottish_deerhound/n02092002_198.jpg\")\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "    img = tf.squeeze(image).numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(3, 3, 9)\n",
    "    image = tf.io.read_file(\"data/Images/n02092002-Scottish_deerhound/n02092002_86.jpg\")\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "    img = tf.squeeze(image).numpy()\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the images don't have the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Breed_full</th>\n",
       "      <th>Breed</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n02111889-Samoyed</td>\n",
       "      <td>Samoyed</td>\n",
       "      <td>218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n02105641-Old_English_sheepdog</td>\n",
       "      <td>Old_English_sheepdog</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n02107908-Appenzeller</td>\n",
       "      <td>Appenzeller</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n02100583-vizsla</td>\n",
       "      <td>vizsla</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n02097298-Scotch_terrier</td>\n",
       "      <td>Scotch_terrier</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>n02109525-Saint_Bernard</td>\n",
       "      <td>Saint_Bernard</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n02088466-bloodhound</td>\n",
       "      <td>bloodhound</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>n02094433-Yorkshire_terrier</td>\n",
       "      <td>Yorkshire_terrier</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>n02086079-Pekinese</td>\n",
       "      <td>Pekinese</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>n02097130-giant_schnauzer</td>\n",
       "      <td>giant_schnauzer</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Breed_full                 Breed  Population\n",
       "0               n02111889-Samoyed               Samoyed         218\n",
       "1  n02105641-Old_English_sheepdog  Old_English_sheepdog         169\n",
       "2           n02107908-Appenzeller           Appenzeller         151\n",
       "3                n02100583-vizsla                vizsla         154\n",
       "4        n02097298-Scotch_terrier        Scotch_terrier         158\n",
       "5         n02109525-Saint_Bernard         Saint_Bernard         170\n",
       "6            n02088466-bloodhound            bloodhound         187\n",
       "7     n02094433-Yorkshire_terrier     Yorkshire_terrier         164\n",
       "8              n02086079-Pekinese              Pekinese         149\n",
       "9       n02097130-giant_schnauzer       giant_schnauzer         157"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fre_df = fre_df.iloc[:10,:]\n",
    "fre_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 3 Custom CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.0 Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_dataset_directory(breeds: list, source_dir: str = \"data/Images\", target_dir: str = \"data/Targets\"):\n",
    "    if os.path.exists(target_dir):\n",
    "        shutil.rmtree(target_dir)\n",
    "    os.mkdir(target_dir)\n",
    "    for breed in breeds:\n",
    "        source = source_dir + \"/\" + breed\n",
    "        target = target_dir + \"/\" + breed\n",
    "        shutil.copytree(source, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_history(history, figsize=(20, 10), metrics: str = \"categorical_accuracy\"):\n",
    "    fix, axs = plt.subplots(2, 1, figsize=figsize, sharex=True)\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.title(\"Loss\")\n",
    "    sns.lineplot(data=history, x=history.index, y=\"loss\", label=\"loss\")\n",
    "    sns.lineplot(data=history, x=history.index, y=\"val_loss\", label=\"val_loss\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.tick_params(labelright=True)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.title(\"Accuracy\")\n",
    "    sns.lineplot(data=history, x=history.index, y=metrics, label=metrics)\n",
    "    sns.lineplot(data=history, x=history.index, y=\"val_\" + metrics, label=\"val_\" + metrics)\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.tick_params(labelright=True)\n",
    "    plt.legend()\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.1 Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 No augmnetation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"custom\"]:\n",
    "\n",
    "    sync_dataset_directory(fre_df[\"Breed_full\"])\n",
    "\n",
    "    ds_train_ = image_dataset_from_directory(\n",
    "        target_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",\n",
    "        image_size=[224, 224],\n",
    "        interpolation=\"nearest\",\n",
    "        batch_size=batch_size,\n",
    "        seed=0,\n",
    "        shuffle=True,\n",
    "        validation_split=0.8,\n",
    "        subset=\"training\"\n",
    "    )\n",
    "\n",
    "    ds_valid_ = image_dataset_from_directory(\n",
    "        target_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",\n",
    "        image_size=[224, 224],\n",
    "        interpolation=\"nearest\",\n",
    "        batch_size=batch_size,\n",
    "        seed=0,\n",
    "        shuffle=True,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\"\n",
    "    )\n",
    "\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    ds_train = (\n",
    "        ds_train_\n",
    "        .map(convert_to_float)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=AUTOTUNE)\n",
    "    )\n",
    "    ds_valid = (\n",
    "        ds_valid_\n",
    "        .map(convert_to_float)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=AUTOTUNE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 3.1.2 With augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_datagen = ImageDataGenerator(\n",
    "#     featurewise_center=True,\n",
    "#     featurewise_std_normalization=True,\n",
    "#     zca_whitening=True,\n",
    "#     rotation_range=90,\n",
    "#     width_shift_range=0.2,\n",
    "#     height_shift_range=0.2,\n",
    "#     horizontal_flip=True,\n",
    "#     vertical_flip=True,\n",
    "#     validation_split=0.2,\n",
    "#     rescale=1./224\n",
    "#     )\n",
    "\n",
    "# valid_datagen = ImageDataGenerator(\n",
    "#     validation_split=0.2,\n",
    "#     rescale=1./224\n",
    "#     )\n",
    "\n",
    "# train_aug = train_datagen.flow_from_directory(\n",
    "#     target_dir,\n",
    "#     target_size=(224, 224),\n",
    "#     subset=\"training\",\n",
    "#     batch_size=32\n",
    "#     )\n",
    "\n",
    "# val_aug = valid_datagen.flow_from_directory(\n",
    "#     target_dir,\n",
    "#     target_size=(224, 224),\n",
    "#     subset=\"validation\",\n",
    "#     batch_size=8\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.2 Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"custom\"]:\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        layers.InputLayer(input_shape=[224, 224, 3]),\n",
    "\n",
    "        # >>> preprocessing <<<\n",
    "        # layers.RandomFlip(mode=\"horizontal_and_vertical\"),\n",
    "        # layers.RandomRotation(factor=0.2),\n",
    "        # layers.RandomTranslation(height_factor=0.2, width_factor=0.2),\n",
    "        # layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "        layers.RandomContrast(factor=0.2),\n",
    "\n",
    "        # >>> base <<<\n",
    "        layers.Conv2D(\n",
    "            filters=16,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            ),\n",
    "        layers.BatchNormalization(axis=3),\n",
    "        layers.Activation(activation=\"relu\"),\n",
    "        layers.MaxPool2D(\n",
    "            pool_size=4,\n",
    "            # strides=2,\n",
    "            padding=\"same\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "\n",
    "        layers.Conv2D(\n",
    "            filters=32,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            ),\n",
    "        layers.BatchNormalization(axis=3),\n",
    "        layers.Activation(activation=\"relu\"),\n",
    "        layers.MaxPool2D(\n",
    "            pool_size=4,\n",
    "            # strides=2,\n",
    "            padding=\"same\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "\n",
    "        layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            ),\n",
    "        layers.BatchNormalization(axis=3),\n",
    "        layers.Activation(activation=\"relu\"),\n",
    "        layers.MaxPool2D(\n",
    "            pool_size=4,\n",
    "            # strides=2,\n",
    "            padding=\"same\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "\n",
    "        layers.Conv2D(\n",
    "            filters=128,\n",
    "            kernel_size=3,\n",
    "            strides=1,\n",
    "            padding=\"same\",\n",
    "            activation=\"relu\",\n",
    "            ),\n",
    "        layers.BatchNormalization(axis=3),\n",
    "        layers.Activation(activation=\"relu\"),\n",
    "        layers.MaxPool2D(\n",
    "            pool_size=4,\n",
    "            # strides=2,\n",
    "            padding=\"same\"\n",
    "        ),\n",
    "        layers.Dropout(rate=0.2),\n",
    "\n",
    "        # layers.GlobalAveragePooling2D(),\n",
    "\n",
    "        # >>> head <<<\n",
    "        layers.Flatten(),\n",
    "\n",
    "        layers.Dropout(rate=0.2),\n",
    "        # layers.BatchNormalization(),\n",
    "        layers.Dense(units=32, activation=\"relu\"),\n",
    "\n",
    "        layers.Dense(units=fre_df.shape[0], activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"custom\"]:\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"categorical_accuracy\"]\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        min_delta=0.001,\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        ds_train,\n",
    "        validation_data=ds_valid,\n",
    "        epochs=50,\n",
    "        # callbacks=[early_stopping]\n",
    "        # verbose=0\n",
    "    )\n",
    "\n",
    "    # history = model.fit_generator(\n",
    "    #     train_aug,\n",
    "    #     validation_data=val_aug,\n",
    "    #     epochs=10,\n",
    "    #     steps_per_epoch=42\n",
    "    #     # callbacks=[early_stopping]\n",
    "    #     # verbose=0\n",
    "    # )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## 3.3 Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"custom\"]:\n",
    "\n",
    "    history_df = pd.DataFrame(data=history.history)\n",
    "    visualize_history(history_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves = record_saves(SAVE_PATH, SAVE_NB, history, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, (key, (model_json, history)) in enumerate(saves.items()):\n",
    "#     if i == 0:\n",
    "#         visualize_history(history)\n",
    "#     else:\n",
    "#         visualize_history(history, figsize=(10, 6))\n",
    "#         # json_parsed = json.loads(model_json)\n",
    "#         # print(json.dumps(json_parsed, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tab_contents = saves.keys()\n",
    "# children = []\n",
    "\n",
    "# for i, (key, (model_json, history)) in enumerate(saves.items()):\n",
    "#     if i == 0:\n",
    "#         visualize_history(history)\n",
    "#     else:\n",
    "#         out = widgets.Output()\n",
    "#         children.append(out)\n",
    "#         with out:\n",
    "#             visualize_history(history, figsize=(10, 6))\n",
    "#             json_parsed = json.loads(model_json)\n",
    "#             print(json.dumps(json_parsed, indent=4, sort_keys=True))\n",
    "\n",
    "# tab = widgets.Tab(children=children)\n",
    "# for i, child in enumerate(children):\n",
    "#     tab.set_title(i, \"tab\"+str(i))\n",
    "\n",
    "# display(tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# 4 Transfer learning CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1677 files belonging to 10 classes.\n",
      "Using 336 files for training.\n",
      "Found 1677 files belonging to 10 classes.\n",
      "Using 335 files for validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-29 13:47:55.645823: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:55.646024: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:55.646217: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:55.646570: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:55.646744: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:55.646878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:55.647102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:55.647288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:939] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-01-29 13:47:55.647399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3295 MB memory:  -> device: 0, name: Quadro P2000, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "if config[\"transfer\"]:\n",
    "\n",
    "    sync_dataset_directory(fre_df[\"Breed_full\"])\n",
    "\n",
    "    ds_train_ = image_dataset_from_directory(\n",
    "        target_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",\n",
    "        image_size=[224, 224],\n",
    "        interpolation=\"nearest\",\n",
    "        batch_size=batch_size,\n",
    "        seed=0,\n",
    "        shuffle=True,\n",
    "        validation_split=0.8,\n",
    "        subset=\"training\"\n",
    "    )\n",
    "\n",
    "    ds_valid_ = image_dataset_from_directory(\n",
    "        target_dir,\n",
    "        labels=\"inferred\",\n",
    "        label_mode=\"categorical\",\n",
    "        image_size=[224, 224],\n",
    "        interpolation=\"nearest\",\n",
    "        batch_size=batch_size,\n",
    "        seed=0,\n",
    "        shuffle=True,\n",
    "        validation_split=0.2,\n",
    "        subset=\"validation\"\n",
    "    )\n",
    "\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    ds_train = (\n",
    "        ds_train_\n",
    "        .map(convert_to_float)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=AUTOTUNE)\n",
    "    )\n",
    "    ds_valid = (\n",
    "        ds_valid_\n",
    "        .map(convert_to_float)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=AUTOTUNE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = Xception(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    pooling=\"avg\",\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "for layer in base.layers:\n",
    "    layer.trainalbe= False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " xception (Functional)       (None, 2048)              20861480  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                65568     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,927,378\n",
      "Trainable params: 20,872,850\n",
      "Non-trainable params: 54,528\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if config[\"transfer\"]:\n",
    "\n",
    "    model = keras.Sequential([\n",
    "        # >>> base <<<\n",
    "        base,\n",
    "\n",
    "        # >>> head <<<\n",
    "        layers.Flatten(),\n",
    "\n",
    "        # layers.Dropout(rate=0.2),\n",
    "        # layers.BatchNormalization(),\n",
    "        layers.Dense(units=32, activation=\"relu\"),\n",
    "\n",
    "        layers.Dense(units=fre_df.shape[0], activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-29 13:52:52.109181: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8300\n",
      "2022-01-29 13:52:52.831697: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.30GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-01-29 13:52:52.958210: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.21GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-01-29 13:52:53.301461: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.64GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-01-29 13:52:53.334197: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.43GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-01-29 13:52:53.559346: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.39GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-01-29 13:52:53.567403: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.45GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-01-29 13:52:53.616705: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.63GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-01-29 13:52:53.626021: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.65GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-01-29 13:52:53.652976: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.16GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-01-29 13:52:53.660260: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.18GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": " A deterministic GPU implementation of DepthwiseConvBackpropFilter is not currently available.\n\t [[node gradient_tape/sequential_1/xception/block14_sepconv2/separable_conv2d/DepthwiseConv2dNativeBackpropFilter\n (defined at /home/alex/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:464)\n]] [Op:__inference_train_function_14454]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradient_tape/sequential_1/xception/block14_sepconv2/separable_conv2d/DepthwiseConv2dNativeBackpropFilter:\nIn[0] sequential_1/xception/block14_sepconv1_act/Relu (defined at /home/alex/.local/lib/python3.8/site-packages/keras/backend.py:4867)\t\nIn[1] gradient_tape/sequential_1/xception/block14_sepconv2/separable_conv2d/Shape_1:\t\nIn[2] gradient_tape/sequential_1/xception/block14_sepconv2/separable_conv2d/Conv2DBackpropInput:\n\nOperation defined at: (most recent call last)\n>>>   File \"/opt/anaconda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/opt/anaconda/lib/python3.8/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/opt/anaconda/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/opt/anaconda/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/opt/anaconda/lib/python3.8/asyncio/events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 450, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2768, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2814, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3012, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3191, in run_ast_nodes\n>>>     if await self.run_code(code, result, async_=asy):\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3251, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_67543/3480656014.py\", line 15, in <module>\n>>>     history = model.fit(\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 816, in train_step\n>>>     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 530, in minimize\n>>>     grads_and_vars = self._compute_gradients(\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 583, in _compute_gradients\n>>>     grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 464, in _get_gradients\n>>>     grads = tape.gradient(loss, var_list, grad_loss)\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m      4\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(\n\u001b[1;32m     10\u001b[0m     min_delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m,\n\u001b[1;32m     11\u001b[0m     patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     12\u001b[0m     restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mds_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_valid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# callbacks=[early_stopping]\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# verbose=0\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 58\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     59\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m:  A deterministic GPU implementation of DepthwiseConvBackpropFilter is not currently available.\n\t [[node gradient_tape/sequential_1/xception/block14_sepconv2/separable_conv2d/DepthwiseConv2dNativeBackpropFilter\n (defined at /home/alex/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:464)\n]] [Op:__inference_train_function_14454]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node gradient_tape/sequential_1/xception/block14_sepconv2/separable_conv2d/DepthwiseConv2dNativeBackpropFilter:\nIn[0] sequential_1/xception/block14_sepconv1_act/Relu (defined at /home/alex/.local/lib/python3.8/site-packages/keras/backend.py:4867)\t\nIn[1] gradient_tape/sequential_1/xception/block14_sepconv2/separable_conv2d/Shape_1:\t\nIn[2] gradient_tape/sequential_1/xception/block14_sepconv2/separable_conv2d/Conv2DBackpropInput:\n\nOperation defined at: (most recent call last)\n>>>   File \"/opt/anaconda/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n>>>     return _run_code(code, main_globals, None,\n>>> \n>>>   File \"/opt/anaconda/lib/python3.8/runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/traitlets/config/application.py\", line 846, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 677, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"/opt/anaconda/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"/opt/anaconda/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"/opt/anaconda/lib/python3.8/asyncio/events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 461, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 450, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 357, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 652, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 353, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 532, in run_cell\n>>>     return super().run_cell(*args, **kwargs)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2768, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2814, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3012, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3191, in run_ast_nodes\n>>>     if await self.run_code(code, result, async_=asy):\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3251, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"/tmp/ipykernel_67543/3480656014.py\", line 15, in <module>\n>>>     history = model.fit(\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/engine/training.py\", line 816, in train_step\n>>>     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 530, in minimize\n>>>     grads_and_vars = self._compute_gradients(\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 583, in _compute_gradients\n>>>     grads_and_vars = self._get_gradients(tape, loss, var_list, grad_loss)\n>>> \n>>>   File \"/home/alex/.local/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py\", line 464, in _get_gradients\n>>>     grads = tape.gradient(loss, var_list, grad_loss)\n>>> "
     ]
    }
   ],
   "source": [
    "if config[\"transfer\"]:\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"categorical_accuracy\"]\n",
    "    )\n",
    "\n",
    "    early_stopping = EarlyStopping(\n",
    "        min_delta=0.001,\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        ds_train,\n",
    "        validation_data=ds_valid,\n",
    "        epochs=50,\n",
    "        # callbacks=[early_stopping]\n",
    "        # verbose=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransfer\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m----> 3\u001b[0m     history_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data\u001b[38;5;241m=\u001b[39m\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory)\n\u001b[1;32m      4\u001b[0m     visualize_history(history_df)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "if config[\"transfer\"]:\n",
    "\n",
    "    history_df = pd.DataFrame(data=history.history)\n",
    "    visualize_history(history_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "586ad1ed5c97141e2437e681efbf1ec0adcd17d830cf5af2ca3d2819e743e158"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
